{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Overview\n",
    "For this assignment, you will be using Python and Spark to perform some graph analysis, using a graph of the Gnutella server network.   In this graph, each node represents a server, and each (directed) edge represents a connection between servers in Gnutella's peer-to-peer network.  The input file for this assignment, `p2p-Gnutella08-adj.txt`, represents the graph as an adjacency list.   Each server (node) is identified by a unique number, and each line in the file gives the adjacency list for a single server.\n",
    "For example, this line:\n",
    "> 91\t243\t1923\t2194\n",
    "\n",
    "gives the adjacency list for server `91`.   It indicates that there are edges from server `91` to servers `243`, `1923`, and `2194`.    According to the Stanford Network Analysis Project, which collected these data, [the graph includes 6301 servers and 20777 edges](http://snap.stanford.edu/data/p2p-Gnutella08.html).\n",
    "\n",
    "As you know from the previous assignment, you must tell the Python interpreter where to find Spark before performing any Spark operations in your program.   So, start by doing that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init(\"/u/cs451/packages/spark\")\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then create a `SparkContext`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(appName=\"YourTest\", master=\"local[2]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Question 1  (6/24 marks):\n",
    "\n",
    "To get warmed up, you should first write Spark code to confirm or determine some basic properties of the Gnutella graph.  Write code that will display answers to the following questions:\n",
    "- How many nodes and edges are there in the graph?  (This should confirm the numbers given above.)\n",
    "- How many edges of each outdegree are there?   That is, how many nodes have no outgoing edges, how many have one outgoing edge, how many have two outgoing edges, and so on?\n",
    "- How many edges of each indegree are there?\n",
    "\n",
    "You should use Spark to answer these questions.   Do *not* load the entire graph into your Python driver program.   It is OK to use a separate set of Spark operations to answer each question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of nodes is 6301\n",
      "The number of edges is 20777\n",
      "\n",
      "3836 nodes have 0 outgoing edges\n",
      "28 nodes have 2 outgoing edges\n",
      "28 nodes have 4 outgoing edges\n",
      "10 nodes have 6 outgoing edges\n",
      "44 nodes have 8 outgoing edges\n",
      "1531 nodes have 10 outgoing edges\n",
      "3 nodes have 12 outgoing edges\n",
      "1 nodes have 34 outgoing edges\n",
      "2 nodes have 14 outgoing edges\n",
      "1 nodes have 48 outgoing edges\n",
      "2 nodes have 18 outgoing edges\n",
      "1 nodes have 46 outgoing edges\n",
      "1 nodes have 22 outgoing edges\n",
      "1 nodes have 24 outgoing edges\n",
      "1 nodes have 28 outgoing edges\n",
      "294 nodes have 1 outgoing edges\n",
      "16 nodes have 3 outgoing edges\n",
      "107 nodes have 5 outgoing edges\n",
      "9 nodes have 7 outgoing edges\n",
      "372 nodes have 9 outgoing edges\n",
      "1 nodes have 11 outgoing edges\n",
      "2 nodes have 13 outgoing edges\n",
      "1 nodes have 47 outgoing edges\n",
      "4 nodes have 17 outgoing edges\n",
      "1 nodes have 19 outgoing edges\n",
      "1 nodes have 41 outgoing edges\n",
      "1 nodes have 25 outgoing edges\n",
      "1 nodes have 29 outgoing edges\n",
      "1 nodes have 31 outgoing edges\n",
      "\n",
      "\n",
      "1287 nodes have 2 ingoing edges\n",
      "559 nodes have 4 ingoing edges\n",
      "1 nodes have 86 ingoing edges\n",
      "3 nodes have 70 ingoing edges\n",
      "76 nodes have 8 ingoing edges\n",
      "37 nodes have 10 ingoing edges\n",
      "23 nodes have 12 ingoing edges\n",
      "2 nodes have 66 ingoing edges\n",
      "13 nodes have 14 ingoing edges\n",
      "1 nodes have 16 ingoing edges\n",
      "1 nodes have 82 ingoing edges\n",
      "4 nodes have 20 ingoing edges\n",
      "1 nodes have 78 ingoing edges\n",
      "1 nodes have 22 ingoing edges\n",
      "1 nodes have 30 ingoing edges\n",
      "2 nodes have 32 ingoing edges\n",
      "227 nodes have 6 ingoing edges\n",
      "1 nodes have 38 ingoing edges\n",
      "1 nodes have 44 ingoing edges\n",
      "2 nodes have 18 ingoing edges\n",
      "2 nodes have 72 ingoing edges\n",
      "1 nodes have 50 ingoing edges\n",
      "1 nodes have 52 ingoing edges\n",
      "1 nodes have 54 ingoing edges\n",
      "2 nodes have 56 ingoing edges\n",
      "3 nodes have 60 ingoing edges\n",
      "1 nodes have 74 ingoing edges\n",
      "2 nodes have 62 ingoing edges\n",
      "2452 nodes have 1 ingoing edges\n",
      "1 nodes have 83 ingoing edges\n",
      "3 nodes have 67 ingoing edges\n",
      "333 nodes have 5 ingoing edges\n",
      "144 nodes have 7 ingoing edges\n",
      "70 nodes have 9 ingoing edges\n",
      "1 nodes have 79 ingoing edges\n",
      "29 nodes have 11 ingoing edges\n",
      "19 nodes have 13 ingoing edges\n",
      "2 nodes have 77 ingoing edges\n",
      "8 nodes have 15 ingoing edges\n",
      "4 nodes have 81 ingoing edges\n",
      "2 nodes have 19 ingoing edges\n",
      "2 nodes have 21 ingoing edges\n",
      "2 nodes have 73 ingoing edges\n",
      "1 nodes have 23 ingoing edges\n",
      "1 nodes have 25 ingoing edges\n",
      "1 nodes have 27 ingoing edges\n",
      "2 nodes have 69 ingoing edges\n",
      "2 nodes have 31 ingoing edges\n",
      "868 nodes have 3 ingoing edges\n",
      "1 nodes have 33 ingoing edges\n",
      "1 nodes have 35 ingoing edges\n",
      "1 nodes have 41 ingoing edges\n",
      "3 nodes have 71 ingoing edges\n",
      "2 nodes have 47 ingoing edges\n",
      "1 nodes have 49 ingoing edges\n",
      "1 nodes have 91 ingoing edges\n",
      "1 nodes have 51 ingoing edges\n",
      "1 nodes have 55 ingoing edges\n",
      "1 nodes have 57 ingoing edges\n",
      "1 nodes have 87 ingoing edges\n",
      "1 nodes have 59 ingoing edges\n",
      "1 nodes have 61 ingoing edges\n",
      "1 nodes have 85 ingoing edges\n",
      "1 nodes have 63 ingoing edges\n",
      "80 nodes have 0 ingoing edges\n"
     ]
    }
   ],
   "source": [
    "#### Your code for Question 1 should go here\n",
    "textFile = sc.textFile(\"p2p-Gnutella08-adj.txt\")\n",
    "M=textFile.count()\n",
    "M1=textFile.flatMap(lambda x:x.split('\\t')[1:])\\\n",
    "           .count()         \n",
    "print('The number of nodes is {0}'.format(M))\n",
    "print('The number of edges is {0}\\n'.format(M1))\n",
    "\n",
    "def outdegree(T):\n",
    "    T1=T.split('\\t')   \n",
    "    return(((len(T1)-1),1))\n",
    "\n",
    "\n",
    "M2=textFile.map(outdegree)\\\n",
    "           .reduceByKey(lambda a,b:a+b)\n",
    "M_out=M2.collect()\n",
    "count1=0\n",
    "for i in range(len(M_out)):\n",
    "    print('{0} nodes have {1} outgoing edges'.format(M_out[i][1],M_out[i][0]))\n",
    "    count1+=M_out[i][1]\n",
    "    \n",
    "\n",
    "print('\\n')\n",
    "\n",
    "def indegree(T):\n",
    "    T1=T.split('\\t')\n",
    "    r=[]\n",
    "    \n",
    "    for i in T1[1:]:\n",
    "        r.append((i,1))   \n",
    "    return r\n",
    "        \n",
    "M3=textFile.flatMap(indegree)\\\n",
    "           .reduceByKey(lambda a,b:a+b)\\\n",
    "           .map(lambda x:(x[1],1))\\\n",
    "           .reduceByKey(lambda a,b:a+b)\n",
    "M_in=M3.collect()\n",
    "count=0\n",
    "for i in range(len(M_in)):\n",
    "    print('{0} nodes have {1} ingoing edges'.format(M_in[i][1],M_in[i][0]))  \n",
    "    count=count+M_in[i][1]\n",
    "print('{0} nodes have {1} ingoing edges'.format(M-count,0))         \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Your main objective for this assignment is to perform *single-source personalized page rank* over the Gnutella graph.  To get started, you should make sure that you have a clear understanding of ordinary (i.e., non-personalized) page rank.  Read the description of page rank in Section 5.3 of [the course textbook](http://mapreduce.cc/).\n",
    "\n",
    "Personalized page rank is like ordinary page rank except:\n",
    "- One node in the graph is designated as the *source* node.   Personalized page rank is performed with respect to that source node.\n",
    "- Personalized page rank is initialized by assigning all probability mass to the source node, and none to the other nodes.   In contrast, ordinary page rank is initialized by giving all nodes the same probability mass.\n",
    "- Whenever personalized page rank makes a random jump, it jumps back to the source node.   In contrast, ordinary page rank may jump to any node.\n",
    "- In personalized page rank, all probability mass lost dangling nodes is put back into the source nodes.  In ordinary page rank, lost mass is distributed evenly over all nodes.\n",
    "\n",
    "#### Question 2  (10/24 marks):\n",
    "\n",
    "Your task is to write a Spark program to perform personalized page rank over the Gnutella graph for a specified number of iterations.   Your program should interactively prompt for two input values:\n",
    "- source node id (a positive integer)\n",
    "- iteration count (a positive integer)\n",
    "\n",
    "Your program should then perform personalized page rank, with respect to the specified source node, over the Gnutella graph, for the specified number of iterations.\n",
    "The output of your analysis should be the 10 nodes with the highest personalized page rank with respect to the given source.   For each of the 10 nodes, report the node's id and page rank value.\n",
    "\n",
    "Be sure that your code is clearly commented.   In particular, for each significant Spark RDD that your code produces, your comments should describe the RDD.   That is, they should indicate how many elements you expect the RDD to have (in terms of graph parameters such as the number of nodes or edges) and what each element consists of.   For example, you might have a comment like this:\n",
    "```\n",
    "# this RDD has one element for each node in the graph\n",
    "# each element is a two-tuple:  (node-id, list of outgoing edges for node-id)\n",
    "```\n",
    "\n",
    "In case you are not able to get this working completely, try to make as much progress as possible.   Produce code that runs and clearly document what it does, so that you can receive some partial credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution to Question 3 here\n",
    "textFile = sc.textFile(\"p2p-Gnutella08-adj.txt\")\n",
    "\n",
    "\n",
    "\n",
    "N=textFile.count()\n",
    "alpha=0.1\n",
    "\n",
    "#(A,(B,C,D))\n",
    "def f1(T):\n",
    "    T1=T.split('\\t')\n",
    "    if len(T1)==1:\n",
    "        return (T1[0],())\n",
    "    else:\n",
    "        return (T1[0],(T1[1:]))\n",
    "\n",
    "def f2(T):\n",
    "    T1=T.split('\\t')\n",
    "    if T1[0]==SourceNode:\n",
    "        return (T1[0],1)\n",
    "    else:\n",
    "        return (T1[0],0)\n",
    "\n",
    "def f3(T):\n",
    "    if T[1][0]==():\n",
    "        if T[0]==SourceNode:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def f4(T):\n",
    "    num=len(T[1][0])\n",
    "    PR=T[1][1]\n",
    "    s=[]\n",
    "    if T[1][0]!=():\n",
    "        for i in T[1][0]:\n",
    "            s.append((i,PR/num))\n",
    "    else:\n",
    "        s.append((T[0],0))########\n",
    "    return s\n",
    "\n",
    "def f5(T):\n",
    "    if T[1][1]==None:\n",
    "        if T[0]==SourceNode:\n",
    "            PR=alpha\n",
    "        else:\n",
    "            PR=0\n",
    "    else:\n",
    "        if T[0]==SourceNode:\n",
    "            PR=alpha+(1-alpha)*T[1][1]\n",
    "        else:\n",
    "            PR=(1-alpha)*T[1][1]\n",
    "    return (T[0],(T[1][0],PR))\n",
    "\n",
    "    \n",
    "def f7(T):\n",
    "    if T[1][0]==():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def f8(T):\n",
    "    if T[1][0]==():\n",
    "        return (T[0],0) \n",
    "    else:\n",
    "        return (T[0],T[1][1])\n",
    "     \n",
    "    \n",
    "def f9(T):\n",
    "    if T[0]==SourceNode:\n",
    "        PR=alpha+(1-alpha)*(Lost_MASS+T[1])\n",
    "    else:\n",
    "        PR=(1-alpha)*T[1]\n",
    "    return (T[0],PR)\n",
    "\n",
    "\n",
    "\n",
    "def f11(T):  \n",
    "    if T[1][1]==None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# this RDD has one element for each node in the graph\n",
    "# each element is a two-tuple:  (node-id, (list of outgoing nodes for node-id))\n",
    "a1=textFile.map(f1).cache()\n",
    "\n",
    "\n",
    "#(A,((B,C,D),1))\n",
    "\n",
    "\n",
    "while True:\n",
    "    SourceNode=input('input a source node(return to quit):')\n",
    "    if len(SourceNode) == 0:\n",
    "        break\n",
    "    else:\n",
    "        Iter=int(input('input a Iteration number:'))\n",
    "        # this RDD has one element for each node in the graph\n",
    "        # each element is a two-tuple:  (node-id, initial page rank value for nodeid)\n",
    "        PR_value_old=textFile.map(f2)\n",
    "        #(A,(([B,C,D]),1))\n",
    "        # this RDD has one element for each node in the graph\n",
    "        # each element is a two-tuple:  (node-id, ((list of outgoing nodes for node-id),initial page rank value for nodeid))\n",
    "        a2=a1.join(PR_value_old)\n",
    "        #First filter those with no outgoing edges,if the source node is one of those, then the probability mass is trapped,can output directly\n",
    "        s=a2.filter(f3).count()\n",
    "        if s==1:\n",
    "            V=PR_value_old.sortBy(lambda x:-x[1]).take(10)\n",
    "        else:\n",
    "            s1=100\n",
    "            for i in range(Iter):             \n",
    "                a31=a1.join(PR_value_old).flatMap(f4).reduceByKey(lambda a,b:a+b) \n",
    "                # Here the reason to outerjoin is that a31 left those nodes with 0 ingoing edges. \n",
    "                a3=a1.leftOuterJoin(a31).map(f5)\n",
    "                #calculate the lost Mass\n",
    "                Lost_MASS=a3.filter(f7).map(lambda x:x[1][1]).reduce(lambda a,b:a+b)\n",
    "                a5=a3.map(f8)\n",
    "                PR_value_new=a5.map(f9)\n",
    "                PR_value_old=PR_value_new\n",
    "            V=PR_value_new.sortBy(lambda x:-x[1]).take(10)\n",
    "        for i in range(10):\n",
    "            print('The node is {0},the page rank value is {1}'.format(V[i][0],V[i][1]))\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Question 3  (4/24 marks):\n",
    "\n",
    "For the previous question, you implemented personalized page rank that ran for a specified number of iterations.  However, it is also common to write iterative algorithms that run until some specified termination condition is reached.\n",
    "For example, for page rank, suppose the $p_i(x)$ represents the probability mass assigned to node $x$ after the $i$th iteration of the algorithm.  ($p_0(x)$ is the initial probability mass of node $x$.)   We define the change of $x$'s probability mass on the $i$th iteration as $\\lvert p_i(x)-p_{i-1}(x) \\rvert$.   Then, we can iterate personalized page rank until the maximum (over all nodes) change is less then a specified threshold, i.e, until all nodes' page ranks have converged.\n",
    "\n",
    "For this question, modify your personalized page rank implementation from Question 2 so that it iterates until the \n",
    "maximum node change is less than $\\frac{0.5}{N}$, where $N$ represents the number of nodes in the graph.\n",
    "This version of your program should take only one input (the source node id).\n",
    "\n",
    "If you were unable to get personalized page rank working in Question 2, replace the code cell below with a text (Markdown) cell, and briefly describe how you *would* have modified your program to incorporate a termination condition based on maximum node change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input a source node(return to quit):999\n",
      "The node is 999,the page rank value is 0.6044085301659551\n",
      "The node is 716,the page rank value is 0.04929019764593394\n",
      "The node is 1001,the page rank value is 0.0490348731009766\n",
      "The node is 1004,the page rank value is 0.04900788063843103\n",
      "The node is 1005,the page rank value is 0.04896962508742925\n",
      "The node is 1002,the page rank value is 0.048967550450710655\n",
      "The node is 174,the page rank value is 0.008300396853917405\n",
      "The node is 390,the page rank value is 0.005154267038611643\n",
      "The node is 124,the page rank value is 0.004730618227324815\n",
      "The node is 176,the page rank value is 0.004725917817696244\n",
      "input a source node(return to quit):\n"
     ]
    }
   ],
   "source": [
    "# your solution to Question 3 here\n",
    "textFile = sc.textFile(\"p2p-Gnutella08-adj.txt\")\n",
    "N=textFile.count()\n",
    "alpha=0.1\n",
    "#(A:(B,C,D))\n",
    "def f1(T):\n",
    "    T1=T.split('\\t')\n",
    "    if len(T1)==1:\n",
    "        return (T1[0],())\n",
    "    else:\n",
    "        return (T1[0],(T1[1:]))\n",
    "\n",
    "#(A:1)\n",
    "def f2(T):\n",
    "    T1=T.split('\\t')\n",
    "    if T1[0]==SourceNode:\n",
    "        return (T1[0],1)\n",
    "    else:\n",
    "        return (T1[0],0)\n",
    "\n",
    "def f3(T):\n",
    "    if T[1][0]==():\n",
    "        if T[0]==SourceNode:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def f4(T):\n",
    "    num=len(T[1][0])\n",
    "    PR=T[1][1]\n",
    "    s=[]\n",
    "    if T[1][0]!=():\n",
    "        for i in T[1][0]:\n",
    "            s.append((i,PR/num))\n",
    "    else:\n",
    "        s.append((T[0],0))\n",
    "    return s\n",
    "\n",
    "def f5(T):\n",
    "    if T[1][1]==None:\n",
    "        if T[0]==SourceNode:\n",
    "            PR=alpha\n",
    "        else:\n",
    "            PR=0\n",
    "    else:\n",
    "        if T[0]==SourceNode:\n",
    "            PR=alpha+(1-alpha)*T[1][1]\n",
    "        else:\n",
    "            PR=(1-alpha)*T[1][1]\n",
    "    return (T[0],(T[1][0],PR))\n",
    "\n",
    "    \n",
    "def f7(T):\n",
    "    if T[1][0]==():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def f8(T):\n",
    "    if T[1][0]==():\n",
    "        return (T[0],0) \n",
    "    else:\n",
    "        return (T[0],T[1][1])\n",
    "     \n",
    "    \n",
    "def f9(T):\n",
    "    if T[0]==SourceNode:\n",
    "        PR=alpha+(1-alpha)*(Lost_MASS+T[1])\n",
    "    else:\n",
    "        PR=(1-alpha)*T[1]\n",
    "    return (T[0],PR)\n",
    "\n",
    "\n",
    "\n",
    "def f11(T):  \n",
    "    if T[1][1]==None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# this RDD has one element for each node in the graph\n",
    "# each element is a two-tuple:  (node-id, (list of outgoing nodes for node-id))\n",
    "a1=textFile.map(f1).cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    SourceNode=input('input a source node(return to quit):')\n",
    "    if len(SourceNode) == 0:\n",
    "        break\n",
    "    else:\n",
    "        # this RDD has one element for each node in the graph\n",
    "        # each element is a two-tuple:  (node-id, initial page rank value for nodeid)\n",
    "        PR_value_old=textFile.map(f2)\n",
    "        #(A,(([B,C,D]),1))\n",
    "        # this RDD has one element for each node in the graph\n",
    "        # each element is a two-tuple:  (node-id, ((list of outgoing nodes for node-id),initial page rank value for nodeid))\n",
    "        a2=a1.join(PR_value_old)\n",
    "        #First filter those with no outgoing edges,if the source node is one of those, then the probability mass is trapped,can output directly\n",
    "        s=a2.filter(f3).count()\n",
    "        if s==1:\n",
    "            V=PR_value_old.sortBy(lambda x:-x[1]).take(10)\n",
    "        else:\n",
    "            s1=100\n",
    "            while s1>=0.5/N:\n",
    "                #(A,((a,b,c),1))              \n",
    "                a31=a1.join(PR_value_old).flatMap(f4).reduceByKey(lambda a,b:a+b)\n",
    "                # Here the reason to outerjoin is that a31 left those nodes with 0 ingoing edges. \n",
    "                a3=a1.leftOuterJoin(a31).map(f5)\n",
    "                #calculate the lost Mass\n",
    "                Lost_MASS=a3.filter(f7).map(lambda x:x[1][1]).reduce(lambda a,b:a+b)\n",
    "                a5=a3.map(f8)\n",
    "                PR_value_new=a5.map(f9)\n",
    "                s1=max(PR_value_old.join(PR_value_new).map(lambda x:abs(x[1][1]-x[1][0])).collect())\n",
    "                PR_value_old=PR_value_new       \n",
    "            V=PR_value_new.sortBy(lambda x:-x[1]).take(10)\n",
    "        for i in range(10):\n",
    "            print('The node is {0},the page rank value is {1}'.format(V[i][0],V[i][1]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Question 4  (4/24 marks):\n",
    "\n",
    "Spark provides the ability to *cache* RDDs.   This is useful when an RDD will be used more than once.   Instead of computing the same RDD multiple times, it can be computed once, cached, and then re-used from the cache.   Read about caching in the Spark textbook, or in the [persistence section of the Spark RDD programming guide](https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-persistence).   Caching can be particularly effective for iterative Spark applications, like those you are writing for this assignment.\n",
    "\n",
    "For this question, go back to the code that you wrote to answer Question 3, and add caching.   (Caching will not affect the functionality of your code, i.e., what it computes.   It should only affect performance.)   Don't worry about different persistence levels.   Just use `cache()`, which caches RDDs at the default persistence level.\n",
    "\n",
    "In addition to adding `cache()` calls in your code, use the text cell below to briefly explain how you decided which RDDs to cache. \n",
    "\n",
    "If you were not able to finish Question 3, add caching annotations to your solution for Question 2 instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Your answer to Question 4:\n",
    "\n",
    "*For Question 3, I basically cached the adjacency list RDD(i.e The 'a1' in Question 3, each line in a1 is like :node_id,(1st outgoing node, 2nd outgoing node,.... )),because for every iteration I have to join it with the PageRank(i.e. The 'PR_value_old' in Question 3) and use it in some other operations during the process. Since it is re-used many times and I only need to compute once, thus here I cached it*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Don't forget to save your workbook!   When you are finished and you are ready to submit your assignment, download your notebook file (.ipynb) from the hub to your machine, and then follow the submission instructions in the assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
