{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Overview\n",
    "For this assignment, you will be using Python and Spark to perform some simple analyses on relational (tabular) data.  You will use Spark to read tabular data from files and then answer simple queries about the data in those tables.\n",
    "\n",
    "In addition to Python and Spark, you will need to use a little bit of SQL.  If you are already familiar with SQL, great.  If not, you will want to spend a short amount of time getting familiar with the basics.   Type \"SQL tutorial\" into your favorite search engine, and you will find many examples of text, interactive and video tutorials.   As a simple starting point, you might also want to look at [these slides](https://cs.uwaterloo.ca/~kmsalem/courses/cs743/F14/slides/sql.pdf), which give a short introduction to SQL.   Even this is much more than you will need for this assignment.\n",
    "\n",
    "You will be working with tabular data based on the schema for the TPC-H benchmark, which is a standard test used to measure the performance of relational database systems.   The schema defines the tables that exist in the database, the information in each table, and relationships between information in one table and information in another.   The TPC-H schema models business information:  customers, orders, parts, suppliers, and so on.   You can find a diagram illustrating the TPC-H schema in the lecture notes.   You can also find it on page 13 of the [TPC-H benchmark specification](http://www.tpc.org/TPC_Documents_Current_Versions/pdf/tpc-h_v2.17.3.pdf).   The TPC-H schema is important for this assignment, so make sure that you keep this schema handy.\n",
    "\n",
    "Finally, for this assignment you will be using Spark in a slightly different way than you have used it so far.  For previous assignments, you have used the original Spark RDD interface.   For this assignment, you will be using the newer Spark interface, which is based on DataFrames.   DataFrames are RDDs in which each element is constrained to have the same structure.   You can think of a DataFrame like a table in a relational database.   Each element of the DataFrame is a row or record in the table.   All records have the same structure.   There is a programming guide for Spark DataFrames [here](https://spark.apache.org/docs/latest/sql-programming-guide.html).  Start with that. There is also a [more detailed guide to the full programming interface](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html).\n",
    "\n",
    "---\n",
    "#### Getting Started\n",
    "To get started, let's initialize Spark, load a couple of the TPC-H tables, and run some simple queries.\n",
    "First, as always, we need to tell the Python interpreter where to find Spark, so run `findspark.init`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'findspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d19f3bb2517e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/u/cs451/packages/spark\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'findspark'"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init(\"/u/cs451/packages/spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we launch Spark.  When you used the RDD interface for previous assignments, you created a `SparkContext` when you launched Spark.   To use Spark SQL and the DataFrame interface, you instead create a `SparkSession`.   You do that as shown in the next cell (run it!).    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"YourTest\").master(\"local[2]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create DataFrames from the TPC-H data files.  We have installed the TPC-H data files in the directory `/u/cs451/data/TPC-H-0.1-TXT/`.   There is one file for each table in the TPC-H database, e.g., `nation.tbl` for the TPC-H Nation table, `customer.tbl` for the TPC-H Customer table, and so on.    These are plain text csv files, with the character | used as a field separator.\n",
    "\n",
    "Create a Spark DataFrame corresponding to the TPC-H Nation table by loading the data from the `nation.tbl` file.   Run the code in the next cell to do this.   After you run this code, `nation_raw` will refer to your new Spark DataFrame.   The Spark `show()` method will display a small (default 20) number of elements from the DataFrame, so that you can inspect them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---+--------------------+----+\n",
      "|_c0|       _c1|_c2|                 _c3| _c4|\n",
      "+---+----------+---+--------------------+----+\n",
      "|  0|   ALGERIA|  0| haggle. carefull...|null|\n",
      "|  1| ARGENTINA|  1|al foxes promise ...|null|\n",
      "|  2|    BRAZIL|  1|y alongside of th...|null|\n",
      "|  3|    CANADA|  1|eas hang ironic, ...|null|\n",
      "|  4|     EGYPT|  4|y above the caref...|null|\n",
      "|  5|  ETHIOPIA|  0|ven packages wake...|null|\n",
      "|  6|    FRANCE|  3|refully final req...|null|\n",
      "|  7|   GERMANY|  3|l platelets. regu...|null|\n",
      "|  8|     INDIA|  2|ss excuses cajole...|null|\n",
      "|  9| INDONESIA|  2| slyly express as...|null|\n",
      "| 10|      IRAN|  4|efully alongside ...|null|\n",
      "| 11|      IRAQ|  4|nic deposits boos...|null|\n",
      "| 12|     JAPAN|  2|ously. final, exp...|null|\n",
      "| 13|    JORDAN|  4|ic deposits are b...|null|\n",
      "| 14|     KENYA|  0| pending excuses ...|null|\n",
      "| 15|   MOROCCO|  0|rns. blithely bol...|null|\n",
      "| 16|MOZAMBIQUE|  0|s. ironic, unusua...|null|\n",
      "| 17|      PERU|  1|platelets. blithe...|null|\n",
      "| 18|     CHINA|  2|c dependencies. f...|null|\n",
      "| 19|   ROMANIA|  3|ular asymptotes a...|null|\n",
      "+---+----------+---+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nation_raw = spark.read.csv(\"/u/cs451/data/TPC-H-0.1-TXT/nation.tbl\",sep='|',inferSchema=True)\n",
    "nation_raw.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have a DataFrame to work with.   The columns of the DataFrame correspond to the fields of the TPC-H Nation table, so have a look at the TPC-H schema diagram to see what you are dealing with.   Column c0 is the NATIONKEY, column c1 is the NAME, c2 is the REGIONKEY, and so on.   Since this is a synthetic database, you'll notice that the data in some of the fields (like the COMMENT field) consists of random words.   That's fine.   You can also ask Spark to tell you about the type of data in each column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'int'),\n",
       " ('_c1', 'string'),\n",
       " ('_c2', 'int'),\n",
       " ('_c3', 'string'),\n",
       " ('_c4', 'string')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nation_raw.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before going on, let's clean this DataFrame up a bit, to make it easier to use.   First, let's assign names to the columns, so that we can remember what information each column holds.   Second, you'll notice that Spark has created an extra final column (filled with `null` values) because each line in the input file ends with a separator character (|).  Let's drop that final column, since we don't need it.   Run the following code to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+--------------------+\n",
      "|NationKey|      Name|RegionKey|             Comment|\n",
      "+---------+----------+---------+--------------------+\n",
      "|        0|   ALGERIA|        0| haggle. carefull...|\n",
      "|        1| ARGENTINA|        1|al foxes promise ...|\n",
      "|        2|    BRAZIL|        1|y alongside of th...|\n",
      "|        3|    CANADA|        1|eas hang ironic, ...|\n",
      "|        4|     EGYPT|        4|y above the caref...|\n",
      "|        5|  ETHIOPIA|        0|ven packages wake...|\n",
      "|        6|    FRANCE|        3|refully final req...|\n",
      "|        7|   GERMANY|        3|l platelets. regu...|\n",
      "|        8|     INDIA|        2|ss excuses cajole...|\n",
      "|        9| INDONESIA|        2| slyly express as...|\n",
      "|       10|      IRAN|        4|efully alongside ...|\n",
      "|       11|      IRAQ|        4|nic deposits boos...|\n",
      "|       12|     JAPAN|        2|ously. final, exp...|\n",
      "|       13|    JORDAN|        4|ic deposits are b...|\n",
      "|       14|     KENYA|        0| pending excuses ...|\n",
      "|       15|   MOROCCO|        0|rns. blithely bol...|\n",
      "|       16|MOZAMBIQUE|        0|s. ironic, unusua...|\n",
      "|       17|      PERU|        1|platelets. blithe...|\n",
      "|       18|     CHINA|        2|c dependencies. f...|\n",
      "|       19|   ROMANIA|        3|ular asymptotes a...|\n",
      "+---------+----------+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nation = nation_raw.toDF('NationKey','Name','RegionKey','Comment','extra').drop('extra').cache()\n",
    "nation.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This style of code should look familar to you.  We started with the `nation_raw` DataFrame and applied a series of DataFrame operations (`toDF`, `drop`, and `cache`).   This is just like the RDD interface, except now we are applying DataFrame operations to DataFrames, instead of RDD operations to RDDs.\n",
    "\n",
    "Next, let's load up the TPC-H Supplier table, and then try performing some queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+---------+---------------+-------+--------------------+\n",
      "|SuppKey|              Name|             Address|NationKey|          Phone|AcctBal|             Comment|\n",
      "+-------+------------------+--------------------+---------+---------------+-------+--------------------+\n",
      "|      1|Supplier#000000001| N kD4on9OM Ipw3,...|       17|27-918-335-1736|5755.94|each slyly above ...|\n",
      "|      2|Supplier#000000002|89eJ5ksX3ImxJQBvx...|        5|15-679-861-2259|4032.68| slyly bold instr...|\n",
      "|      3|Supplier#000000003|q1,G3Pj6OjIuUYfUo...|        1|11-383-516-1199| 4192.4|blithely silent r...|\n",
      "|      4|Supplier#000000004|Bk7ah4CK8SYQTepEm...|       15|25-843-787-7479|4641.08|riously even requ...|\n",
      "|      5|Supplier#000000005|   Gcdm2rJRzl5qlTVzc|       11|21-151-690-3663|-283.84|. slyly regular p...|\n",
      "|      6|Supplier#000000006|        tQxuVm7s7CnK|       14|24-696-997-4969|1365.79|final accounts. r...|\n",
      "|      7|Supplier#000000007|s,4TicNGB4uO6PaSq...|       23|33-990-965-2201|6820.35|s unwind silently...|\n",
      "|      8|Supplier#000000008|9Sq4bBH2FQEmaFOoc...|       17|27-498-742-3860|7627.85|al pinto beans. a...|\n",
      "|      9|Supplier#000000009|1KhUgZegwM3ua7dsY...|       10|20-403-398-8662|5302.37|s. unusual, even ...|\n",
      "|     10|Supplier#000000010|  Saygah3gYWMp72i PY|       24|34-852-489-8585|3891.91|ing waters. regul...|\n",
      "|     11|Supplier#000000011|    JfwTs,LZrV, M,9C|       18|28-613-996-1505|3393.08|y ironic packages...|\n",
      "|     12|Supplier#000000012|         aLIW  q0HYd|        8|18-179-925-7181|1432.69|al packages nag a...|\n",
      "|     13|Supplier#000000013|HK71HQyWoqRWOX8GI...|        3|13-727-620-7813|9107.22|requests engage r...|\n",
      "|     14|Supplier#000000014|     EXsnO5pTNj4iZRm|       15|25-656-247-5058|9189.82|l accounts boost....|\n",
      "|     15|Supplier#000000015|olXVbNBfVzRqgokr1...|        8|18-453-357-6394| 308.56| across the furio...|\n",
      "|     16|Supplier#000000016|YjP5C55zHDXL7LalK...|       22|32-822-502-4215|2972.26|ously express ide...|\n",
      "|     17|Supplier#000000017|c2d,ESHRSkK3WYnxp...|       19|29-601-884-9219|1687.81|eep against the f...|\n",
      "|     18|Supplier#000000018|    PGGVE5PWAMwKDZw |       16|26-729-551-1115|7040.82|accounts snooze s...|\n",
      "|     19|Supplier#000000019|edZT3es,nBFD8lBXT...|       24|34-278-310-2731|6150.38|refully final fox...|\n",
      "|     20|Supplier#000000020|iybAE,RmTymrZVYaF...|        3|13-715-945-6730| 530.82|n, ironic ideas w...|\n",
      "+-------+------------------+--------------------+---------+---------------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "supplier_raw = spark.read.csv(\"/u/cs451/data/TPC-H-0.1-TXT/supplier.tbl\",sep='|',inferSchema=True).drop(\"_c7\")\n",
    "supplier = supplier_raw.toDF(\"SuppKey\",\"Name\",\"Address\",\"NationKey\",\"Phone\",\"AcctBal\",\"Comment\").cache()\n",
    "supplier.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Writing Queries\n",
    "There are two equivalent ways of writing queries over Spark DataFrames.   The first way is to assign a \"view name\" to the DataFrame, and then write SQL queries referring to those view names using the `sql` operation.  \n",
    "\n",
    "The code below gives the view names \"nation\" and \"supplier\" to the two DataFrames we've already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplier.createOrReplaceTempView(\"supplier\")\n",
    "\n",
    "nation.createOrReplaceTempView(\"nation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can write SQL queries that refer to the \"supplier\" and \"nation\" views as tables.   For example, suppose we want to see the names and addresses of suppliers who have account balances above 9900.00:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+-------+\n",
      "|              Name|             Address|AcctBal|\n",
      "+------------------+--------------------+-------+\n",
      "|Supplier#000000049|     Nvq 6macF4GtJvz|9915.24|\n",
      "|Supplier#000000234|iMrk7HUD87at3IIh4rBi| 9957.0|\n",
      "|Supplier#000000693|S,mnHfsroFOVieQGd...|9956.55|\n",
      "|Supplier#000000855|ekQwhb9fh5VGIvMBJ...|9964.88|\n",
      "|Supplier#000000892|j6prA4M3sX9a9xHem...|9993.46|\n",
      "+------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q1_result = spark.sql(\"select Name, Address, AcctBal from supplier where AcctBal > 9900.00\")\n",
    "q1_result.show()\n",
    "\n",
    "\n",
    "\n",
    "q1_result=spark.sql(\"select Name,Address,AcctBal from supplier where AcctBal > 9900.00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the `sql` command runs the SQL query against the supplier table.   It returns the query result as a new DataFrame, which `q1_result` refers to.\n",
    "\n",
    "Instead of writing your queries in SQL and running them using `sql`, it is possible to do the same thing by applying a sequence of DataFrame operations to the input DataFrames, as you did when you were using the RDD interface in the previous assignments.    For example, to answer the same query that we just answered using SQL, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_resultB = supplier.filter(\"AcctBal > 9900.00\").select('Name','Address','AcctBal')\n",
    "q1_resultB.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both methods should give the same result.   Internally, Spark handles both similarly.   For this assignment, you'll be asked to try out both methods.\n",
    "\n",
    "Now it is time for you to try writing your own queries.\n",
    "\n",
    "---\n",
    "#### Question 1 (2/25 marks)\n",
    "In the cell below, write a query that will return the ORDERKEY, ORDERDATE, and TOTALPRICE of the five orders with the highest TOTALPRICE.   Express your query in SQL, and use `sql` to execute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+\n",
      "|ORDERKEY|           ORDERDATE|TOTALPRICE|\n",
      "+--------+--------------------+----------+\n",
      "|  279812|1994-02-19 00:00:...| 479129.21|\n",
      "|  370726|1996-09-29 00:00:...|  460099.4|\n",
      "|   66659|1993-10-15 00:00:...| 458396.42|\n",
      "|  253639|1998-01-23 00:00:...| 456532.89|\n",
      "|  502886|1994-04-12 00:00:...| 456423.88|\n",
      "+--------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your solution to Question 1 here\n",
    "orders_raw = spark.read.csv(\"/u/cs451/data/TPC-H-0.1-TXT/orders.tbl\",sep='|',inferSchema=True).drop(\"_c9\")\n",
    "orders = orders_raw.toDF(\"ORDERKEY\",\"CUSTKEY\",\"ORDERSTATUS\",\"TOTALPRICE\",\"ORDERDATE\",\"ORDERPRIORITY\",\"CLERK\",\"SHIPPRIORITY\",\"COMMENT\").cache()\n",
    "orders.createOrReplaceTempView('Orders')\n",
    "q1_orders=spark.sql('select ORDERKEY, ORDERDATE,TOTALPRICE from Orders order by TOTALPRICE Desc limit 5')\n",
    "q1_orders.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Question 2 (2/25 marks)\n",
    "In the cell below, answer the same query you answered in Question 1, but this time do not use the `sql` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+\n",
      "|ORDERKEY|           ORDERDATE|TOTALPRICE|\n",
      "+--------+--------------------+----------+\n",
      "|  279812|1994-02-19 00:00:...| 479129.21|\n",
      "|  370726|1996-09-29 00:00:...|  460099.4|\n",
      "|   66659|1993-10-15 00:00:...| 458396.42|\n",
      "|  253639|1998-01-23 00:00:...| 456532.89|\n",
      "|  502886|1994-04-12 00:00:...| 456423.88|\n",
      "+--------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your solution to Question 2 here\n",
    "q1_orders2=orders.orderBy(orders.TOTALPRICE.desc()).limit(5).select('ORDERKEY', 'ORDERDATE','TOTALPRICE')\n",
    "q1_orders2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Question 3 (3/25 marks)\n",
    "In the cell below, write code that will prompt for a Customer key, and then return the customer's name as well as the ORDERDATE and TOTALPRICE of that customer's most recent order.   Express the query in SQL, and use `sql` to execute it.   You will need to use information from multiple tables to generate your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input a custmomer key10\n",
      "+------------------+--------------------+----------+\n",
      "|              NAME|           ORDERDATE|TOTALPRICE|\n",
      "+------------------+--------------------+----------+\n",
      "|Customer#000000010|1998-03-29 00:00:...|   89751.5|\n",
      "+------------------+--------------------+----------+\n",
      "\n",
      "Please input a custmomer key\n"
     ]
    }
   ],
   "source": [
    "# Your solution to Question 3 here\n",
    "customer_raw = spark.read.csv(\"/u/cs451/data/TPC-H-0.1-TXT/customer.tbl\",sep='|',inferSchema=True).drop(\"_c8\")\n",
    "customer = customer_raw.toDF(\"CUSTKEY\",\"NAME\",\"ADDRESS\",\"NATIONKEY\",\"PHONE\",\"ACCTBAL\",\"MKTSEGMENT\",\"COMMENT\").cache()\n",
    "customer.createOrReplaceTempView('customer')\n",
    "while True:\n",
    "    Customer_key=input('Please input a custmomer key')\n",
    "    if len(Customer_key)==0:\n",
    "        break\n",
    "    else:\n",
    "        Customer_key=int(Customer_key)\n",
    "        q2_result=spark.sql('select NAME,ORDERDATE,TOTALPRICE from Orders inner join customer on Orders.CUSTKEY=customer.CUSTKEY where customer.CUSTKEY={0} order by ORDERDATE Desc limit 1'.format(Customer_key)).show(1)\n",
    "\n",
    "             \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Question 4 (3/25 marks)\n",
    "In the cell below, answer the same query you answered in Question 3, but this time do not use the `sql` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input a custmomer key:10\n",
      "+------------------+--------------------+----------+\n",
      "|              Name|           ORDERDATE|TOTALPRICE|\n",
      "+------------------+--------------------+----------+\n",
      "|Customer#000000010|1998-03-29 00:00:...|   89751.5|\n",
      "+------------------+--------------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "Please input a custmomer key:\n"
     ]
    }
   ],
   "source": [
    "# Your solution to Question 4 here\n",
    "while True:\n",
    "    Customer_key=input('Please input a custmomer key:')\n",
    "    if len(Customer_key)==0:\n",
    "        break\n",
    "    else:\n",
    "        Customer_key=int(Customer_key)\n",
    "        q2_resultb=customer.join(orders,orders.CUSTKEY==customer.CUSTKEY).filter(orders.CUSTKEY==Customer_key).select('Name','ORDERDATE','TOTALPRICE').orderBy(orders.ORDERDATE.desc()).show(1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Question 5 (5/25 marks)\n",
    "In the cell below, write code that will prompt for a Nation name.  (Assume that nation names are unique in the Nation table.)   Report the number of distinct parts supplied by suppliers that are located in the given nation.\n",
    "\n",
    "You may answer this question with or without using `sql` - whichever you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input a nation name:CANADA\n",
      "2799\n",
      "Please input a nation name:\n"
     ]
    }
   ],
   "source": [
    "# Your solution to Question 5 here\n",
    "Partsupp_raw = spark.read.csv(\"/u/cs451/data/TPC-H-0.1-TXT/partsupp.tbl\",sep='|',inferSchema=True).drop(\"_c5\")\n",
    "Partsupp = Partsupp_raw.toDF(\"PARTKEY\",\"SUPPKEY\",\"AVAILQTY\",\"SUPPLYCOST\",\"COMMENT\").cache()\n",
    "Partsupp.createOrReplaceTempView('Partsupp')\n",
    "while True:\n",
    "    Nation_name=input('Please input a nation name:')\n",
    "    if len(Nation_name)==0:\n",
    "        break\n",
    "    else:\n",
    "        q5_result=supplier.join(Partsupp,supplier.SuppKey==Partsupp.SUPPKEY).join(nation,supplier.NationKey==nation.NationKey).filter(nation.Name==Nation_name).select('PARTKEY').distinct().count()\n",
    "        print(q5_result)\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Question 6 (5/25 marks)\n",
    "In the cell below, write code that will prompt for a BRAND, like those that appear in the Parts table.  Given the BRAND,\n",
    "report, for each nation, a count of that nation's suppliers of parts having that brand.   Your output should be a table of nations and their supplier counts. Each supplier should be counted at most once in a nation's total, even if that supplier produces multiple parts with the given brand. \n",
    "\n",
    "You may answer this question with or without using `sql` - whichever you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input a Brand_input:Brand#13\n",
      "+--------------+-----+\n",
      "|          Name|count|\n",
      "+--------------+-----+\n",
      "|         JAPAN|   39|\n",
      "|        JORDAN|   28|\n",
      "|UNITED KINGDOM|   39|\n",
      "|        CANADA|   36|\n",
      "|       ALGERIA|   34|\n",
      "|       MOROCCO|   37|\n",
      "|        FRANCE|   34|\n",
      "|    MOZAMBIQUE|   34|\n",
      "|         CHINA|   52|\n",
      "|  SAUDI ARABIA|   47|\n",
      "|     ARGENTINA|   36|\n",
      "|          IRAN|   39|\n",
      "|        RUSSIA|   46|\n",
      "|         INDIA|   46|\n",
      "|         KENYA|   35|\n",
      "|          IRAQ|   41|\n",
      "|       VIETNAM|   39|\n",
      "|      ETHIOPIA|   31|\n",
      "|     INDONESIA|   44|\n",
      "| UNITED STATES|   36|\n",
      "|       ROMANIA|   33|\n",
      "|       GERMANY|   49|\n",
      "|        BRAZIL|   42|\n",
      "|         EGYPT|   40|\n",
      "|          PERU|   39|\n",
      "+--------------+-----+\n",
      "\n",
      "Please input a Brand_input:\n"
     ]
    }
   ],
   "source": [
    "# Your solution to Question 6 here\n",
    "Part_raw = spark.read.csv(\"/u/cs451/data/TPC-H-0.1-TXT/part.tbl\",sep='|',inferSchema=True).drop(\"_c9\")\n",
    "Part = Part_raw.toDF(\"PARTKEY\",\"NAME\",\"MFGR\",\"BRAND\",\"TYPE\",\"SIZE\",\"CONTAINER\",\"RETAILPRICE\",\"COMMENT\").cache()\n",
    "Part.createOrReplaceTempView('Part')\n",
    "while True:\n",
    "    Brand_input=input('Please input a Brand_input:')\n",
    "    if len(Brand_input)==0:\n",
    "        break\n",
    "    else:\n",
    "        q6_result=Part.join(Partsupp,Part.PARTKEY==Partsupp.PARTKEY,'inner').drop(Partsupp.PARTKEY).select('BRAND','PARTKEY','SUPPKEY').join(supplier,Partsupp.SUPPKEY==supplier.SuppKey,'inner').\\\n",
    "        drop(supplier.SuppKey).select('BRAND','NationKey','SUPPKEY','PARTKEY').join(nation,supplier.NationKey==nation.NationKey,'inner').drop(nation.NationKey).select('Name','BRAND','SUPPKEY').distinct().filter(Part.BRAND==Brand_input).\\\n",
    "        groupBy('Name').count()\n",
    "        q6_result1=q6_result.show(q6_result.count())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Question 7 (5/25 marks)\n",
    "In the cell below, write code that will prompt for a Nation name.   Report, for each year, the total number of orders placed by customers from the specified Nation.\n",
    "\n",
    "You may answer this question with or without using `sql` - whichever you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input a Nation name:CANADA\n",
      "+----+-----+\n",
      "|Year|count|\n",
      "+----+-----+\n",
      "|1992|  982|\n",
      "|1993|  900|\n",
      "|1994|  912|\n",
      "|1995|  932|\n",
      "|1996|  940|\n",
      "|1997|  921|\n",
      "|1998|  595|\n",
      "+----+-----+\n",
      "\n",
      "Please input a Nation name:CHINA\n",
      "+----+-----+\n",
      "|Year|count|\n",
      "+----+-----+\n",
      "|1992|  925|\n",
      "|1993|  913|\n",
      "|1994|  975|\n",
      "|1995|  889|\n",
      "|1996|  951|\n",
      "|1997|  917|\n",
      "|1998|  571|\n",
      "+----+-----+\n",
      "\n",
      "Please input a Nation name:\n"
     ]
    }
   ],
   "source": [
    "# Your solution to Question 7 here\n",
    "from pyspark.sql.functions import *\n",
    "Part_raw = spark.read.csv(\"/u/cs451/data/TPC-H-0.1-TXT/part.tbl\",sep='|',inferSchema=True).drop(\"_c9\")\n",
    "Part = Part_raw.toDF(\"PARTKEY\",\"NAME\",\"MFGR\",\"BRAND\",\"TYPE\",\"SIZE\",\"CONTAINER\",\"RETAILPRICE\",\"COMMENT\").cache()\n",
    "while True:\n",
    "    input_Nation=input('Please input a Nation name:')\n",
    "    if len(input_Nation)==0:\n",
    "        break\n",
    "    else:\n",
    "        q7_result=nation.join(customer,nation.NationKey==customer.NATIONKEY).drop(customer.NAME).join(orders,customer.CUSTKEY==orders.CUSTKEY).withColumn('Year',year(orders.ORDERDATE)).select('Year','Name','ORDERDATE').filter(nation.Name==input_Nation).groupBy('Year').count().orderBy('Year')\n",
    "        q7_result1=q7_result.show(q7_result.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Don't forget to save your workbook!   When you are finished and you are ready to submit your assignment, download your notebook file (.ipynb) from the hub to your machine, and then follow the submission instructions in the assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
